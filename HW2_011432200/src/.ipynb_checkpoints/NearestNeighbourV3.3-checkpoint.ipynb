{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import spatial\n",
    "\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn import cross_validation\n",
    "import heapq\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "st = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18506\n",
      "18506\n",
      "18506\n"
     ]
    }
   ],
   "source": [
    "with open(\"TestData/train.dat\", \"r\") as fh:\n",
    "    #with open(\"TestData/Test/training_out.dat\", \"r\") as fh:\n",
    "    linesOfTrainData = fh.readlines()\n",
    "print(len(linesOfTrainData))\n",
    "\n",
    "#with open(\"TestData/format.dat\", \"r\") as fh:\n",
    "with open(\"TestData/Test/format_out.dat\", \"r\") as fh:\n",
    "    linesOfFormat = fh.readlines()\n",
    "print(len(linesOfFormat))\n",
    "\n",
    "with open(\"TestData/test.dat\", \"r\") as fh:\n",
    "#with open(\"TestData/Test/test_out.dat\", \"r\") as fh:\n",
    "    linesOfTestData = fh.readlines()\n",
    "print(len(linesOfTestData))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ##stopwords and stemming for Training\n",
    "\n",
    "# print (\"First line before cleanup from Training Data: \",linesOfTrainData[0])\n",
    "\n",
    "# stops = set(stopwords.words('english'))\n",
    "\n",
    "# linesOfTrainDataAfterPreProcessing=[]\n",
    "# for line in linesOfTrainData:\n",
    "#     newLine = []\n",
    "#     for w in line.split():\n",
    "#         if w.lower()not in stops:\n",
    "#             newLine.append(w)     #for stopwords  \n",
    "#     linesOfTrainDataAfterPreProcessing.append(\" \".join(newLine))\n",
    "\n",
    "\n",
    "# linesOfTrainDataAfterSteming = []\n",
    "# for line in linesOfTrainDataAfterPreProcessing:\n",
    "#     newLine = line\n",
    "#     for w in newLine.split():\n",
    "#             newLine = newLine.replace(w, st.stem(w)) # for stemming\n",
    "#     linesOfTrainDataAfterSteming.append(newLine)\n",
    "    \n",
    "# #Not working    \n",
    "# linesOfTrainDataAfterStemingWithoutPunctuation = []\n",
    "# for line in linesOfTrainDataAfterSteming:\n",
    "#     newLine = line\n",
    "#     for w in newLine.split():\n",
    "#          if w in string.punctuation:\n",
    "#             newLine = newLine.translate(string.punctuation)\n",
    "#             #newLine = newLine.replace(w,re.sub(r'[^\\w\\s]()','',w)) # for Punctuation \n",
    "#     linesOfTrainDataAfterStemingWithoutPunctuation.append(newLine)    \n",
    "\n",
    "\n",
    "# linesOfTrainData = linesOfTrainDataAfterStemingWithoutPunctuation\n",
    "\n",
    "# print (\"\\n\\nFirst line after cleanup from Training Data: \",linesOfTrainData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line before cleanup from Training Data:  +1\tThis book is such a life saver.  It has been so helpful to be able to go back to track trends, answer pediatrician questions, or communicate with each other when you are up at different times of the night with a newborn.  I think it is one of those things that everyone should be required to have before they leave the hospital.  We went through all the pages of the newborn version, then moved to the infant version, and will finish up the second infant book (third total) right as our baby turns 1.  See other things that are must haves for baby at [...]\n",
      "\n",
      "\n",
      "\n",
      "First line after cleanup from Training Data:    book life saver helpful able go back track trends answer pediatrician questions communicate different times night newborn think one things everyone required leave hospital went pages newborn version moved infant version finish second infant book third total right baby turns   See things must haves baby \n"
     ]
    }
   ],
   "source": [
    "                        ##stopwords and stemming for Training\n",
    "\n",
    "print (\"First line before cleanup from Training Data: \",linesOfTrainData[0])\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "linesOfTrainDataAfterPreProcessing=[]\n",
    "for line in linesOfTrainData:\n",
    "    newLine = []\n",
    "    for w in line.split():\n",
    "        if w.lower()not in stops:\n",
    "            newLine.append(w)     #for stopwords\n",
    "            finalLine = \" \".join(newLine)\n",
    "            finalLine = finalLine.replace(w, st.stem(w)) #for stemming\n",
    "            finalLine = finalLine.translate(str.maketrans('','',string.punctuation)) #for punctuation\n",
    "            finalLine = re.sub(\"\\d+\", \" \", finalLine)# for numbers\n",
    "    linesOfTrainDataAfterPreProcessing.append(finalLine)\n",
    "\n",
    "\n",
    "linesOfTrainData = linesOfTrainDataAfterPreProcessing\n",
    "\n",
    "print (\"\\n\\nFirst line after cleanup from Training Data: \",linesOfTrainData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ##stopwords and stemming for Testdata\n",
    "\n",
    "# print (\"First line before cleanup from Test Data: \",linesOfTestData[0])\n",
    "\n",
    "# stops = set(stopwords.words('english'))\n",
    "\n",
    "# linesOfTestDataAfterPreProcessing=[]\n",
    "# for line in linesOfTestData:\n",
    "#     newLine = []\n",
    "#     for w in line.split():\n",
    "#         if w.lower()not in stops:\n",
    "#             newLine.append(w)       \n",
    "#     linesOfTestDataAfterPreProcessing.append(\" \".join(newLine)) # for identifting words\n",
    "    \n",
    "\n",
    "# linesOfTestDataAfterSteming = []\n",
    "# for line in linesOfTestDataAfterPreProcessing:\n",
    "#     newLine = line\n",
    "#     for w in newLine.split():\n",
    "#             newLine = newLine.replace(w, st.stem(w)) # for stemming\n",
    "#     linesOfTestDataAfterSteming.append(newLine)\n",
    "    \n",
    "# linesOfTestDataAfterStemingWithoutPunctuation = []\n",
    "# for line in linesOfTestDataAfterSteming:\n",
    "#     newLine = line\n",
    "#     for w in newLine.split():\n",
    "#          if w in string.punctuation:\n",
    "#             newLine = newLine.translate(string.punctuation)\n",
    "#             #newLine = newLine.replace(w,re.sub(r'[^\\w\\s]()','',w)) # for Punctuation \n",
    "#     linesOfTestDataAfterStemingWithoutPunctuation.append(newLine)\n",
    "    \n",
    "    \n",
    "\n",
    "# linesOfTestData = linesOfTestDataAfterStemingWithoutPunctuation\n",
    "\n",
    "# print (\"\\n\\nFirst line after cleanup from Test Data: \",linesOfTestData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line before cleanup from Testing Data:  Perfect for new parents. We were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. Made life easier when the doctor would ask questions about habits because we had it all right there!\n",
      "\n",
      "\n",
      "\n",
      "First line after cleanup from Testing Data:  Perfect new parents able keep track babys feeding sleep diaper change schedule first two half months life Made life easier doctor would ask questions habits right there\n"
     ]
    }
   ],
   "source": [
    "                        ##stopwords and stemming for Training\n",
    "\n",
    "print (\"First line before cleanup from Testing Data: \",linesOfTestData[0])\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "linesOfTestDataAfterPreProcessing=[]\n",
    "for line in linesOfTestData:\n",
    "    newLine = []\n",
    "    for w in line.split():\n",
    "        if w.lower()not in stops:\n",
    "            newLine.append(w)     #for stopwords\n",
    "            finalLine = \" \".join(newLine)\n",
    "            finalLine = finalLine.replace(w, st.stem(w)) #for stemming\n",
    "            finalLine = finalLine.translate(str.maketrans('','',string.punctuation)) #for punctuation\n",
    "            finalLine = re.sub(\"\\d+\", \" \", finalLine)# for numbers\n",
    "    linesOfTestDataAfterPreProcessing.append(finalLine)\n",
    "\n",
    "\n",
    "linesOfTestData = linesOfTestDataAfterPreProcessing\n",
    "\n",
    "print (\"\\n\\nFirst line after cleanup from Testing Data: \",linesOfTestData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 42541.\n"
     ]
    }
   ],
   "source": [
    "# get a frequency count for all words in the Training docs\n",
    "\n",
    "\n",
    "\n",
    "wordsInTrainingSet = set()\n",
    "for d in linesOfTrainData:\n",
    "    for w in d.split():\n",
    "        if w == \"+1\" or w == \"-1\":\n",
    "            continue\n",
    "        else:\n",
    "            if w not in wordsInTrainingSet:\n",
    "                wordsInTrainingSet.add(w)\n",
    "                #wordsList.append(w)\n",
    "            #else:\n",
    "             #   wordsInTrainingSet[w] += 1\n",
    "print(\"Number of unique words: %d.\" % len(wordsInTrainingSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 42279.\n"
     ]
    }
   ],
   "source": [
    "# get a frequency count for all words in the Test docs\n",
    "wordsInTestSet  = set()\n",
    "for d in linesOfTestData:\n",
    "    for w in d.split():\n",
    "        if w not in wordsInTestSet:\n",
    "            wordsInTestSet.add(w)\n",
    "            #wordsList.append(w)\n",
    "        #else:\n",
    "         #   wordsInTestSet[w] += 1\n",
    "print(\"Number of unique words: %d.\" % len(wordsInTestSet))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 20050.\n"
     ]
    }
   ],
   "source": [
    "wordsInSet = set()\n",
    "wordsList= []\n",
    "\n",
    "wordsInSet =  set.intersection(wordsInTrainingSet,wordsInTestSet)\n",
    "wordsList = list(wordsInSet)\n",
    "\n",
    "print(\"Number of unique words: %d.\" % len(wordsInSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "4010\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(wordsInSet)\n",
    "\n",
    "top_percentile = 0.2 # 0.2 Precentile\n",
    "features = c.most_common(int(round(len(c)*top_percentile)))\n",
    "features_counts = len(features)\n",
    "\n",
    "print(type(features))\n",
    "print(features_counts)\n",
    "\n",
    "#print (wordsList)\n",
    "\n",
    "#top_percentile = 0.5\n",
    "# features_counts = len(words.most_common(int(round(len(words)*top_percentile))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('washing', 1)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#print(features)\n",
    "print(features[:1][0])\n",
    "print(features[:1][0][1])\n",
    "\n",
    "featureList= []\n",
    "\n",
    "\n",
    "for feature in features:\n",
    "    featureList.append(feature[0])\n",
    "    \n",
    "   \n",
    "#print(featureList)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #Buid matrix\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    dim = len(featureList)\n",
    "    feature_set = set(featureList[:dim])\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        set_d = set(d)\n",
    "        \n",
    "        d = list(set.intersection(feature_set,set_d))\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        set_d = set(d)\n",
    "        \n",
    "        d = list(set.intersection(feature_set,set_d))\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #Buid matrix\n",
    "\n",
    "# from collections import Counter\n",
    "# from scipy.sparse import csr_matrix\n",
    "# def build_matrix(docs):\n",
    "#     r\"\"\" Build sparse matrix from a list of documents, \n",
    "#     each of which is a list of word/terms in the document.  \n",
    "#     \"\"\"\n",
    "#     nrows = len(docs)\n",
    "#     idx = {}\n",
    "#     tid = 0\n",
    "#     nnz = 0\n",
    "#     for d in docs:\n",
    "#         nnz += len(set(d))\n",
    "#         for w in d:\n",
    "#             if w not in idx:\n",
    "#                 idx[w] = tid\n",
    "#                 tid += 1\n",
    "#     ncols = len(idx)\n",
    "#     print(idx)\n",
    "        \n",
    "#     # set up memory\n",
    "#     ind = np.zeros(nnz, dtype=np.int)\n",
    "#     val = np.zeros(nnz, dtype=np.double)\n",
    "#     ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "#     i = 0  # document ID / row counter\n",
    "#     n = 0  # non-zero counter\n",
    "#     # transfer values\n",
    "#     for d in docs:\n",
    "#         cnt = Counter(d)\n",
    "#         keys = list(k for k,_ in cnt.most_common())\n",
    "#         l = len(keys)\n",
    "#         for j,k in enumerate(keys):\n",
    "#             ind[j+n] = idx[k]\n",
    "#             val[j+n] = cnt[k]\n",
    "#         ptr[i+1] = ptr[i] + l\n",
    "#         n += l\n",
    "#         i += 1\n",
    "            \n",
    "#     mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "#     mat.sort_indices()\n",
    "    \n",
    "#     return mat\n",
    "\n",
    "\n",
    "# def csr_info(mat, name=\"\", non_empy=False):\n",
    "#     r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "#     report number of non-empty rows and cols as well\n",
    "#     \"\"\"\n",
    "#     if non_empy:\n",
    "#         print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "#                 name, mat.shape[0], \n",
    "#                 sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "#                 for i in range(mat.shape[0])), \n",
    "#                 mat.shape[1], len(np.unique(mat.indices)), \n",
    "#                 len(mat.data)))\n",
    "#     else:\n",
    "#         print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "#                 mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 18506, ncols 4010, nnz 151735]\n",
      " [nrows 18506, ncols 4010, nnz 151588]\n"
     ]
    }
   ],
   "source": [
    "#print(linesOfTrainData[:1])\n",
    "docslinesOfTrainData = [l.split() for l in linesOfTrainData]\n",
    "matOflinesOfTrainData  = build_matrix(docslinesOfTrainData)\n",
    "csr_info(matOflinesOfTrainData)\n",
    "\n",
    "#print (matOflinesOfTrainData[:1])\n",
    "\n",
    "#print(linesOfTestData[:1])\n",
    "docslinesOfTestData = [l.split() for l in linesOfTestData]\n",
    "matOflinesOfTestData  = build_matrix(docslinesOfTestData)\n",
    "csr_info(matOflinesOfTestData)\n",
    "#print (matOflinesOfTestData[:1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# scale matrix and normalize its rows\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]] \n",
      "\n",
      "mat2: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]] \n",
      "\n",
      "mat3: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "mat2linesOfTrainData = csr_idf(matOflinesOfTrainData, copy=True)\n",
    "mat3linesOfTrainData = csr_l2normalize(mat2linesOfTrainData, copy=True)\n",
    "print(\"mat1:\", matOflinesOfTrainData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat2:\", mat2linesOfTrainData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat3:\", mat3linesOfTrainData[15,:20].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1: [[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]] \n",
      "\n",
      "mat2: [[ 0.          0.          0.          0.          0.          2.35278119\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]] \n",
      "\n",
      "mat3: [[ 0.          0.          0.          0.          0.          0.56026111\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mat2linesOfTestData = csr_idf(matOflinesOfTestData, copy=True)\n",
    "mat3linesOfTestData = csr_l2normalize(mat2linesOfTestData, copy=True)\n",
    "print(\"mat1:\", matOflinesOfTestData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat2:\", mat2linesOfTestData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat3:\", mat3linesOfTestData[15,:20].todense())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "linesOfTrainData_Transformed = mat3linesOfTrainData\n",
    "linesOfTestData_Transformed = mat3linesOfTestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(linesOfTrainData, linesOfTestData, test_size= 0.1,train_size= 0.9, random_state=42)\n",
    "\n",
    "\n",
    "# vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "# #linesOfTrainData_Transformed =  vectorizer.fit_transform(features_train)\n",
    "# linesOfTrainData_Transformed =  vectorizer.fit_transform(linesOfTrainData)\n",
    "\n",
    "# print (len(vectorizer.vocabulary_)) # after loading vocab from 1 source\n",
    "# print(vectorizer.vocabulary_['younger'])\n",
    "# linesOfTrainData_Transformed =  vectorizer.fit_transform(linesOfTestData)\n",
    "# print (len(vectorizer.vocabulary_)) # after loading vocab from 2nd source\n",
    "# print(vectorizer.vocabulary_['younger'])\n",
    "\n",
    "\n",
    "# #linesOfTestData_Transformed = vectorizer.transform(features_test)\n",
    "# linesOfTestData_Transformed = vectorizer.transform(linesOfTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['book', 'life', 'saver', 'helpful', 'able', 'go', 'back', 'track', 'trends', 'answer', 'pediatrician', 'questions', 'communicate', 'different', 'times', 'night', 'newborn', 'think', 'one', 'things', 'everyone', 'required', 'leave', 'hospital', 'went', 'pages', 'newborn', 'version', 'moved', 'infant', 'version', 'finish', 'second', 'infant', 'book', 'third', 'total', 'right', 'baby', 'turns', 'See', 'things', 'must', 'haves', 'baby']]\n",
      "-------\n",
      "  (0, 0)\t0.391576903565\n",
      "  (0, 1)\t0.618255353735\n",
      "  (0, 2)\t0.44726347477\n",
      "  (0, 3)\t0.332210620256\n",
      "  (0, 4)\t0.392452970556\n"
     ]
    }
   ],
   "source": [
    "print(docslinesOfTrainData[:1]) \n",
    "print('-------')\n",
    "print(linesOfTrainData_Transformed[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Perfect', 'new', 'parents', 'able', 'keep', 'track', 'babys', 'feeding', 'sleep', 'diaper', 'change', 'schedule', 'first', 'two', 'half', 'months', 'life', 'Made', 'life', 'easier', 'doctor', 'would', 'ask', 'questions', 'habits', 'right', 'there']]\n",
      "-------\n",
      "  (0, 0)\t0.615545942367\n",
      "  (0, 1)\t0.415259062774\n",
      "  (0, 2)\t0.320598313819\n",
      "  (0, 3)\t0.382838545706\n",
      "  (0, 4)\t0.376063621604\n",
      "  (0, 5)\t0.240604707399\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(docslinesOfTestData[:1])\n",
    "print('-------')\n",
    "print(linesOfTestData_Transformed[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selector = SelectPercentile(f_classif, percentile=10)\n",
    "# selector.fit(linesOfTrainData_Transformed, labels_train) #labels_train\n",
    "# linesOfTrainData_Transformed = selector.transform(linesOfTrainData_Transformed)\n",
    "# linesOfTestData_Transformed = selector.transform(linesOfTestData_Transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def CalculateCosine(vt,vs):\n",
    "#         dotProduct = vt.dot(np.transpose(vS))#######\n",
    "#         lengtht = np.linalg.norm(vt.data)\n",
    "#         lengthS = np.linalg.norm(vS.data)\n",
    "\n",
    "#         #handle exceptions\n",
    "\n",
    "#         if lengthS!=0 and lengtht!=0 :\n",
    "#             cosineSimilarityValue= dotProduct/(lengtht*lengthS)\n",
    "#         else:\n",
    "#             cosineSimilarityValue= 0\n",
    "#         return cosineSimilarityValue\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def CalculateCosine(vt,vs):\n",
    "        cosineSimilarityValue = cosine_similarity(vt,vs)\n",
    "        return cosineSimilarityValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cosineSimilarityValue = CalculateCosine(linesOfTestData_Transformed,linesOfTrainData_Transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18506\n"
     ]
    }
   ],
   "source": [
    "print(len(cosineSimilarityValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count :  18506\n",
      "--The End--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = open('TestData/Test/format_out.dat', 'w')\n",
    "count = 0\n",
    "for row in cosineSimilarityValue:\n",
    "\n",
    "    \n",
    "    #kneighbours = heapq.nlargest(5, row)\n",
    "    k=72\n",
    "    partitioned_row_byindex = np.argpartition(-row, k)  \n",
    "    similar_index = partitioned_row_byindex[:k]\n",
    "    \n",
    "    #print(similar_index)\n",
    "    \n",
    "    neighbourReviewTypeList = []\n",
    "    neighbourReviewTypeNegative = 0\n",
    "    neighbourReviewTypePositive = 0\n",
    "    \n",
    "    #print(\"@@@@@\",+count)\n",
    "    for index in similar_index:\n",
    "\n",
    "        if linesOfTrainData[index].strip()[0] == '-':\n",
    "            #neighbourReviewTypeList.append(\"-1\")\n",
    "            neighbourReviewTypeNegative+=1\n",
    "        elif linesOfTrainData[index].strip()[0] == '+':\n",
    "            #neighbourReviewTypeList.append(\"+1\")\n",
    "            neighbourReviewTypePositive+=1\n",
    "            \n",
    "    \n",
    "    if neighbourReviewTypeNegative > neighbourReviewTypePositive:\n",
    "        f.write('-1\\n')\n",
    "        count+=1\n",
    "    else:\n",
    "        f.write('+1\\n')\n",
    "        count+=1\n",
    "\n",
    "print(\"count : \",count)\n",
    "print(\"--The End--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18506\n"
     ]
    }
   ],
   "source": [
    "#with open(\"TestData/format.dat\", \"r\") as fh:\n",
    "with open(\"TestData/Test/format_out.dat\", \"r\") as fh:\n",
    "    linesOfFormat = fh.readlines()\n",
    "print(len(linesOfFormat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

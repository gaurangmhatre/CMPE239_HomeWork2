{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import spatial\n",
    "\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn import cross_validation\n",
    "import heapq\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "st = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18506\n",
      "0\n",
      "18506\n"
     ]
    }
   ],
   "source": [
    "with open(\"TestData/train.dat\", \"r\") as fh:\n",
    "    #with open(\"TestData/Test/training_out.dat\", \"r\") as fh:\n",
    "    linesOfTrainData = fh.readlines()\n",
    "print(len(linesOfTrainData))\n",
    "\n",
    "#with open(\"TestData/format.dat\", \"r\") as fh:\n",
    "with open(\"TestData/Test/format_out.dat\", \"r\") as fh:\n",
    "    linesOfFormat = fh.readlines()\n",
    "print(len(linesOfFormat))\n",
    "\n",
    "with open(\"TestData/test.dat\", \"r\") as fh:\n",
    "#with open(\"TestData/Test/test_out.dat\", \"r\") as fh:\n",
    "    linesOfTestData = fh.readlines()\n",
    "print(len(linesOfTestData))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line before cleanup from Training Data:  +1\tThis book is such a life saver.  It has been so helpful to be able to go back to track trends, answer pediatrician questions, or communicate with each other when you are up at different times of the night with a newborn.  I think it is one of those things that everyone should be required to have before they leave the hospital.  We went through all the pages of the newborn version, then moved to the infant version, and will finish up the second infant book (third total) right as our baby turns 1.  See other things that are must haves for baby at [...]\n",
      "\n",
      "\n",
      "\n",
      "First line after cleanup from Training Data:  +1\tthi book lif saver.  help abl go back track trends, answ pedy questions, commun diff tim night newborn.  think on thing everyon requir leav hospital.  went pag newborn version, mov inf version, fin second inf book (third total) right baby turn 1.  see thing must hav baby [...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##stopwords and stemming for Training\n",
    "\n",
    "print (\"First line before cleanup from Training Data: \",linesOfTrainData[0])\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "linesOfTrainDataAfterPreProcessing=[]\n",
    "for line in linesOfTrainData:\n",
    "    newLine = line\n",
    "    for w in newLine.split():\n",
    "        if w.lower() in stops:\n",
    "            newLine = newLine.replace(' '+w+' ', ' ') # for identifting words\n",
    "    linesOfTrainDataAfterPreProcessing.append(newLine)\n",
    "\n",
    "\n",
    "linesOfTrainDataAfterSteming = []\n",
    "for line in linesOfTrainDataAfterPreProcessing:\n",
    "    newLine = line\n",
    "    for w in newLine.split():\n",
    "            newLine = newLine.replace(w, st.stem(w)) # for stemming\n",
    "    linesOfTrainDataAfterSteming.append(newLine)\n",
    "\n",
    "linesOfTrainData = linesOfTrainDataAfterSteming\n",
    "\n",
    "print (\"\\n\\nFirst line after cleanup from Training Data: \",linesOfTrainData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line before cleanup from Test Data:  Perfect for new parents. We were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. Made life easier when the doctor would ask questions about habits because we had it all right there!\n",
      "\n",
      "\n",
      "\n",
      "First line after cleanup from Test Data:  perfect new parents. abl keep track baby's feeding, sleep diap chang schedule first two half month lif. mad lif easy doct would ask quest habit right there!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##stopwords and stemming for Testdata\n",
    "\n",
    "print (\"First line before cleanup from Test Data: \",linesOfTestData[0])\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "linesOfTestDataAfterPreProcessing=[]\n",
    "for line in linesOfTestData:\n",
    "    newLine = line\n",
    "    for w in newLine.split():\n",
    "        if w.lower() in stops:\n",
    "            newLine = newLine.replace(' '+w+' ', ' ') # for identifting words\n",
    "    linesOfTestDataAfterPreProcessing.append(newLine)\n",
    "\n",
    "\n",
    "linesOfTestDataAfterSteming = []\n",
    "for line in linesOfTestDataAfterPreProcessing:\n",
    "    newLine = line\n",
    "    for w in newLine.split():\n",
    "            newLine = newLine.replace(w, st.stem(w)) # for stemming\n",
    "    linesOfTestDataAfterSteming.append(newLine)\n",
    "\n",
    "linesOfTestData = linesOfTestDataAfterSteming\n",
    "\n",
    "print (\"\\n\\nFirst line after cleanup from Test Data: \",linesOfTestData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Buid matrix\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 18506, ncols 63755, nnz 853128]\n",
      " [nrows 18506, ncols 63475, nnz 831193]\n"
     ]
    }
   ],
   "source": [
    "#print(linesOfTrainData[:1])\n",
    "docslinesOfTrainData = [l.split() for l in linesOfTrainData]\n",
    "matOflinesOfTrainData  = build_matrix(docslinesOfTrainData)\n",
    "csr_info(matOflinesOfTrainData)\n",
    "\n",
    "#print (matOflinesOfTrainData[:1])\n",
    "\n",
    "#print(linesOfTestData[:1])\n",
    "docslinesOfTestData = [l.split() for l in linesOfTestData]\n",
    "matOflinesOfTestData  = build_matrix(docslinesOfTestData)\n",
    "csr_info(matOflinesOfTestData)\n",
    "#print (matOflinesOfTestData[:1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale matrix and normalize its rows\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1: [[ 1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  2.  1.  2.  1.  1.  1.  1.\n",
      "   1.  2.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]] \n",
      "\n",
      "mat2: [[ 0.61550991  0.          0.          0.          0.          2.72335093\n",
      "   0.          0.          0.          5.59174378  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]] \n",
      "\n",
      "mat3: [[ 0.01839653  0.          0.          0.          0.          0.08139626\n",
      "   0.          0.          0.          0.16712757  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "mat2linesOfTrainData = csr_idf(matOflinesOfTrainData, copy=True)\n",
    "mat3linesOfTrainData = csr_l2normalize(mat2linesOfTrainData, copy=True)\n",
    "print(\"mat1:\", matOflinesOfTrainData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat2:\", mat2linesOfTrainData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat3:\", mat3linesOfTrainData[15,:20].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1: [[ 0.  0.  0.  1.  2.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]] \n",
      "\n",
      "mat2: [[ 0.          0.          0.          2.66655838  4.03424195  0.          0.\n",
      "   6.16228864  2.80834414  0.          0.          7.52326519  0.          0.\n",
      "   0.          0.          0.          2.55762726  0.          0.        ]] \n",
      "\n",
      "mat3: [[ 0.          0.          0.          0.08845562  0.13382471  0.          0.\n",
      "   0.20441671  0.09315897  0.          0.          0.2495633   0.          0.\n",
      "   0.          0.          0.          0.08484214  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mat2linesOfTestData = csr_idf(matOflinesOfTestData, copy=True)\n",
    "mat3linesOfTestData = csr_l2normalize(mat2linesOfTestData, copy=True)\n",
    "print(\"mat1:\", matOflinesOfTestData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat2:\", mat2linesOfTestData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat3:\", mat3linesOfTestData[15,:20].todense())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linesOfTrainData_Transformed = mat3linesOfTrainData\n",
    "linesOfTestData_Transformed = mat3linesOfTestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(linesOfTrainData, linesOfTestData, test_size= 0.1,train_size= 0.9, random_state=42)\n",
    "\n",
    "\n",
    "# vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "# #linesOfTrainData_Transformed =  vectorizer.fit_transform(features_train)\n",
    "# linesOfTrainData_Transformed =  vectorizer.fit_transform(linesOfTrainData)\n",
    "\n",
    "# print (len(vectorizer.vocabulary_)) # after loading vocab from 1 source\n",
    "# print(vectorizer.vocabulary_['younger'])\n",
    "# linesOfTrainData_Transformed =  vectorizer.fit_transform(linesOfTestData)\n",
    "# print (len(vectorizer.vocabulary_)) # after loading vocab from 2nd source\n",
    "# print(vectorizer.vocabulary_['younger'])\n",
    "\n",
    "\n",
    "# #linesOfTestData_Transformed = vectorizer.transform(features_test)\n",
    "# linesOfTestData_Transformed = vectorizer.transform(linesOfTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['+1', 'thi', 'book', 'lif', 'saver.', 'help', 'abl', 'go', 'back', 'track', 'trends,', 'answ', 'pedy', 'questions,', 'commun', 'diff', 'tim', 'night', 'newborn.', 'think', 'on', 'thing', 'everyon', 'requir', 'leav', 'hospital.', 'went', 'pag', 'newborn', 'version,', 'mov', 'inf', 'version,', 'fin', 'second', 'inf', 'book', '(third', 'total)', 'right', 'baby', 'turn', '1.', 'see', 'thing', 'must', 'hav', 'baby', '[...]']]\n",
      "-------\n",
      "  (0, 0)\t0.0167483821515\n",
      "  (0, 1)\t0.0557516468449\n",
      "  (0, 2)\t0.286906915696\n",
      "  (0, 3)\t0.115849204935\n",
      "  (0, 4)\t0.187247198179\n",
      "  (0, 5)\t0.0741039604008\n",
      "  (0, 6)\t0.0718485390377\n",
      "  (0, 7)\t0.0685156302294\n",
      "  (0, 8)\t0.0591368801156\n",
      "  (0, 9)\t0.152154595795\n",
      "  (0, 10)\t0.248506155532\n",
      "  (0, 11)\t0.162029465048\n",
      "  (0, 12)\t0.172225091423\n",
      "  (0, 13)\t0.237473214453\n",
      "  (0, 14)\t0.185851477849\n",
      "  (0, 15)\t0.0732442593683\n",
      "  (0, 16)\t0.0482421079443\n",
      "  (0, 17)\t0.0866211691737\n",
      "  (0, 18)\t0.138973998171\n",
      "  (0, 19)\t0.0612729294095\n",
      "  (0, 20)\t0.0334597922937\n",
      "  (0, 21)\t0.118325466802\n",
      "  (0, 22)\t0.12621531236\n",
      "  (0, 23)\t0.115436915521\n",
      "  (0, 24)\t0.0954733728541\n",
      "  (0, 25)\t0.166990540498\n",
      "  (0, 26)\t0.0893426310805\n",
      "  (0, 27)\t0.163785595691\n",
      "  (0, 28)\t0.0980471786616\n",
      "  (0, 29)\t0.371702955698\n",
      "  (0, 30)\t0.0807415549477\n",
      "  (0, 31)\t0.193151956339\n",
      "  (0, 32)\t0.0844404317752\n",
      "  (0, 33)\t0.0867278077718\n",
      "  (0, 34)\t0.248506155532\n",
      "  (0, 35)\t0.267367092883\n",
      "  (0, 36)\t0.0709563089149\n",
      "  (0, 37)\t0.065529441019\n",
      "  (0, 38)\t0.0788642188446\n",
      "  (0, 39)\t0.162029465048\n",
      "  (0, 40)\t0.0698614827022\n",
      "  (0, 41)\t0.0984277524287\n",
      "  (0, 42)\t0.155065369296\n",
      "  (0, 43)\t0.202118969853\n"
     ]
    }
   ],
   "source": [
    "print(docslinesOfTrainData[:1]) \n",
    "print('-------')\n",
    "print(linesOfTrainData_Transformed[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['perfect', 'new', 'parents.', 'abl', 'keep', 'track', \"baby's\", 'feeding,', 'sleep', 'diap', 'chang', 'schedule', 'first', 'two', 'half', 'month', 'lif.', 'mad', 'lif', 'easy', 'doct', 'would', 'ask', 'quest', 'habit', 'right', 'there!']]\n",
      "-------\n",
      "  (0, 0)\t0.11740359889\n",
      "  (0, 1)\t0.121595409068\n",
      "  (0, 2)\t0.259086074095\n",
      "  (0, 3)\t0.115198278833\n",
      "  (0, 4)\t0.0871418629181\n",
      "  (0, 5)\t0.246893318528\n",
      "  (0, 6)\t0.136030218307\n",
      "  (0, 7)\t0.266217702365\n",
      "  (0, 8)\t0.121323580969\n",
      "  (0, 9)\t0.114896991304\n",
      "  (0, 10)\t0.119024505898\n",
      "  (0, 11)\t0.325013398624\n",
      "  (0, 12)\t0.089876220791\n",
      "  (0, 13)\t0.101495730537\n",
      "  (0, 14)\t0.165541847676\n",
      "  (0, 15)\t0.0986327427568\n",
      "  (0, 16)\t0.347081632017\n",
      "  (0, 17)\t0.110492333814\n",
      "  (0, 18)\t0.184260015333\n",
      "  (0, 19)\t0.0671917727854\n",
      "  (0, 20)\t0.259086074095\n",
      "  (0, 21)\t0.0569521716817\n",
      "  (0, 22)\t0.188079466727\n",
      "  (0, 23)\t0.257247451558\n",
      "  (0, 24)\t0.307496856341\n",
      "  (0, 25)\t0.116459465855\n",
      "  (0, 26)\t0.280532698074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(docslinesOfTestData[:1])\n",
    "print('-------')\n",
    "print(linesOfTestData_Transformed[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selector = SelectPercentile(f_classif, percentile=10)\n",
    "# selector.fit(linesOfTrainData_Transformed, labels_train) #labels_train\n",
    "# linesOfTrainData_Transformed = selector.transform(linesOfTrainData_Transformed)\n",
    "# linesOfTestData_Transformed = selector.transform(linesOfTestData_Transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def CalculateCosine(vt,vs):\n",
    "#         dotProduct = vt.dot(np.transpose(vS))#######\n",
    "#         lengtht = np.linalg.norm(vt.data)\n",
    "#         lengthS = np.linalg.norm(vS.data)\n",
    "\n",
    "#         #handle exceptions\n",
    "\n",
    "#         if lengthS!=0 and lengtht!=0 :\n",
    "#             cosineSimilarityValue= dotProduct/(lengtht*lengthS)\n",
    "#         else:\n",
    "#             cosineSimilarityValue= 0\n",
    "#         return cosineSimilarityValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def CalculateCosine(vt,vs):\n",
    "        cosineSimilarityValue = cosine_similarity(vt,vs)\n",
    "        return cosineSimilarityValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# f = open('TestData/Test/format_out.dat', 'w')\n",
    "# for vt in linesOfTestData_Transformed:\n",
    "#     cosineSimilarityValues=[]\n",
    "#     for vS in linesOfTrainData_Transformed:\n",
    "#         cosineSimilarityValue = CalculateCosine(vt,vS)\n",
    "#         cosineSimilarityValues.append(cosineSimilarityValue)\n",
    "\n",
    "#     kneighbours = heapq.nlargest(3, cosineSimilarityValues)\n",
    "#     #kneighbours = sp.sparse.csr_matrix.max(cosineSimilarityValues)\n",
    "\n",
    "#     neighbourReviewTypeList = []\n",
    "#     neighbourReviewTypeNegative = 0\n",
    "#     neighbourReviewTypePositive = 0\n",
    "#     print(\"@@@@@\")\n",
    "#     for neighbour in kneighbours:\n",
    "#         index = cosineSimilarityValues.index(neighbour.data[0])\n",
    "#         print(\"####\",index)\n",
    "#         if linesOfTrainData[index].strip()[0] == '-':\n",
    "#             #neighbourReviewTypeList.append(\"-1\")\n",
    "#             neighbourReviewTypeNegative+=1\n",
    "#         elif linesOfTrainData[index].strip()[0] == '+':\n",
    "#             #neighbourReviewTypeList.append(\"+1\")\n",
    "#             neighbourReviewTypePositive+=1\n",
    "\n",
    "\n",
    "#     if neighbourReviewTypeNegative > neighbourReviewTypePositive:\n",
    "#         f.write('-1\\n')\n",
    "#     else:\n",
    "#         f.write('+1\\n')\n",
    "\n",
    "# print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18506\n",
      "[array([[ 0.69337345]]), array([[ 0.67230148]]), array([[ 0.65705224]])]\n",
      "@@@@@\n",
      "#### 820\n",
      "#### 3567\n",
      "#### 11181\n",
      "18506\n",
      "[array([[ 0.6205295]]), array([[ 0.61564223]]), array([[ 0.59382624]])]\n",
      "@@@@@\n",
      "#### 9185\n",
      "#### 11635\n",
      "#### 4214\n",
      "18506\n",
      "[array([[ 0.66874799]]), array([[ 0.66256147]]), array([[ 0.66100515]])]\n",
      "@@@@@\n",
      "#### 9328\n",
      "#### 11214\n",
      "#### 111\n",
      "18506\n",
      "[array([[ 0.8139218]]), array([[ 0.8111136]]), array([[ 0.80382476]])]\n",
      "@@@@@\n",
      "#### 11985\n",
      "#### 277\n",
      "#### 9480\n",
      "18506\n",
      "[array([[ 0.61302298]]), array([[ 0.60860692]]), array([[ 0.60176035]])]\n",
      "@@@@@\n",
      "#### 4214\n",
      "#### 10288\n",
      "#### 10264\n",
      "18506\n",
      "[array([[ 0.94300464]]), array([[ 0.93545677]]), array([[ 0.9234104]])]\n",
      "@@@@@\n",
      "#### 4727\n",
      "#### 3621\n",
      "#### 8443\n",
      "18506\n",
      "[array([[ 0.55342758]]), array([[ 0.53843594]]), array([[ 0.53776602]])]\n",
      "@@@@@\n",
      "#### 897\n",
      "#### 6000\n",
      "#### 1151\n",
      "18506\n",
      "[array([[ 0.88193679]]), array([[ 0.87611395]]), array([[ 0.86455994]])]\n",
      "@@@@@\n",
      "#### 7314\n",
      "#### 4490\n",
      "#### 11181\n",
      "18506\n",
      "[array([[ 0.75371328]]), array([[ 0.74744419]]), array([[ 0.73710371]])]\n",
      "@@@@@\n",
      "#### 5473\n",
      "#### 16777\n",
      "#### 6935\n",
      "18506\n",
      "[array([[ 0.82725944]]), array([[ 0.82725944]]), array([[ 0.82725944]])]\n",
      "@@@@@\n",
      "#### 4959\n",
      "#### 4959\n",
      "#### 4959\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-9be79bf7de6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcosineSimilarityValues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinesOfTrainData_Transformed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mcosineSimilarityValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalculateCosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mcosineSimilarityValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosineSimilarityValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-e5c7478124f6>\u001b[0m in \u001b[0;36mCalculateCosine\u001b[0;34m(vt, vs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCalculateCosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mcosineSimilarityValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcosineSimilarityValue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=dtype,\n\u001b[0;32m--> 109\u001b[0;31m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n\u001b[0m\u001b[1;32m    110\u001b[0m         Y = check_array(Y, accept_sparse='csr', dtype=dtype,\n\u001b[1;32m    111\u001b[0m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 380\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    266\u001b[0m                           % spmatrix.format)\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = open('TestData/Test/format_out.dat', 'w')\n",
    "for vt in linesOfTestData_Transformed:\n",
    "    cosineSimilarityValues=[]\n",
    "    for vS in linesOfTrainData_Transformed:\n",
    "        cosineSimilarityValue = CalculateCosine(vt,vS)\n",
    "        cosineSimilarityValues.append(cosineSimilarityValue)\n",
    "    \n",
    "    print(len(cosineSimilarityValues))\n",
    "    kneighbours = heapq.nlargest(3, cosineSimilarityValues)\n",
    "    #kneighbours = sp.sparse.csr_matrix.max(cosineSimilarityValues)\n",
    "    print(kneighbours)\n",
    "    neighbourReviewTypeList = []\n",
    "    neighbourReviewTypeNegative = 0\n",
    "    neighbourReviewTypePositive = 0\n",
    "    print(\"@@@@@\")\n",
    "    for neighbour in kneighbours:\n",
    "        index = cosineSimilarityValues.index(neighbour)\n",
    "        print(\"####\",index)\n",
    "        if linesOfTrainData[index].strip()[0] == '-':\n",
    "            #neighbourReviewTypeList.append(\"-1\")\n",
    "            neighbourReviewTypeNegative+=1\n",
    "        elif linesOfTrainData[index].strip()[0] == '+':\n",
    "            #neighbourReviewTypeList.append(\"+1\")\n",
    "            neighbourReviewTypePositive+=1\n",
    "\n",
    "\n",
    "    if neighbourReviewTypeNegative > neighbourReviewTypePositive:\n",
    "        f.write('-1\\n')\n",
    "    else:\n",
    "        f.write('+1\\n')\n",
    "\n",
    "print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

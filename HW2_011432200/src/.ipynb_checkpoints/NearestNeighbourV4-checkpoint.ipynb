{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import spatial\n",
    "\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn import cross_validation\n",
    "import heapq\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "st = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18506\n",
      "0\n",
      "18506\n"
     ]
    }
   ],
   "source": [
    "with open(\"TestData/train.dat\", \"r\") as fh:\n",
    "    #with open(\"TestData/Test/training_out.dat\", \"r\") as fh:\n",
    "    linesOfTrainData = fh.readlines()\n",
    "print(len(linesOfTrainData))\n",
    "\n",
    "#with open(\"TestData/format.dat\", \"r\") as fh:\n",
    "with open(\"TestData/Test/format_out.dat\", \"r\") as fh:\n",
    "    linesOfFormat = fh.readlines()\n",
    "print(len(linesOfFormat))\n",
    "\n",
    "with open(\"TestData/test.dat\", \"r\") as fh:\n",
    "#with open(\"TestData/Test/test_out.dat\", \"r\") as fh:\n",
    "    linesOfTestData = fh.readlines()\n",
    "print(len(linesOfTestData))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line before cleanup from Training Data:  +1\tThis book is such a life saver.  It has been so helpful to be able to go back to track trends, answer pediatrician questions, or communicate with each other when you are up at different times of the night with a newborn.  I think it is one of those things that everyone should be required to have before they leave the hospital.  We went through all the pages of the newborn version, then moved to the infant version, and will finish up the second infant book (third total) right as our baby turns 1.  See other things that are must haves for baby at [...]\n",
      "\n",
      "\n",
      "\n",
      "First line after cleanup from Training Data:  +1 book lif saver. help abl go back track trends, answ pedy questions, commun diff tim night newborn. think on thing everyon requir leav hospital. went pag newborn version, mov inf version, fin second inf book (third total) right baby turn 1. see thing must hav baby [...]\n"
     ]
    }
   ],
   "source": [
    "##stopwords and stemming for Training\n",
    "\n",
    "print (\"First line before cleanup from Training Data: \",linesOfTrainData[0])\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "linesOfTrainDataAfterPreProcessing=[]\n",
    "for line in linesOfTrainData:\n",
    "    newLine = []\n",
    "    for w in line.split():\n",
    "        if w.lower()not in stops:\n",
    "            newLine.append(w)     #for stopwords  \n",
    "    linesOfTrainDataAfterPreProcessing.append(\" \".join(newLine))\n",
    "\n",
    "\n",
    "linesOfTrainDataAfterSteming = []\n",
    "for line in linesOfTrainDataAfterPreProcessing:\n",
    "    newLine = line\n",
    "    for w in newLine.split():\n",
    "            newLine = newLine.replace(w, st.stem(w)) # for stemming\n",
    "    linesOfTrainDataAfterSteming.append(newLine)\n",
    "    \n",
    "#Not working    \n",
    "linesOfTrainDataAfterStemingWithoutPunctuation = []\n",
    "for line in linesOfTrainDataAfterSteming:\n",
    "    newLine = line\n",
    "    for w in newLine.split():\n",
    "         if w in string.punctuation:\n",
    "            newLine = newLine.translate(string.punctuation)\n",
    "            #newLine = newLine.replace(w,re.sub(r'[^\\w\\s]()','',w)) # for Punctuation \n",
    "    linesOfTrainDataAfterStemingWithoutPunctuation.append(newLine)    \n",
    "\n",
    "\n",
    "linesOfTrainData = linesOfTrainDataAfterStemingWithoutPunctuation\n",
    "\n",
    "print (\"\\n\\nFirst line after cleanup from Training Data: \",linesOfTrainData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line before cleanup from Test Data:  Perfect for new parents. We were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. Made life easier when the doctor would ask questions about habits because we had it all right there!\n",
      "\n",
      "\n",
      "\n",
      "First line after cleanup from Test Data:  perfect new parents. abl keep track baby's feeding, sleep diap chang schedule first two half month lif. mad lif easy doct would ask quest habit right there!\n"
     ]
    }
   ],
   "source": [
    "##stopwords and stemming for Testdata\n",
    "\n",
    "print (\"First line before cleanup from Test Data: \",linesOfTestData[0])\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "linesOfTestDataAfterPreProcessing=[]\n",
    "for line in linesOfTestData:\n",
    "    newLine = []\n",
    "    for w in line.split():\n",
    "        if w.lower()not in stops:\n",
    "            newLine.append(w)       \n",
    "    linesOfTestDataAfterPreProcessing.append(\" \".join(newLine)) # for identifting words\n",
    "    \n",
    "\n",
    "linesOfTestDataAfterSteming = []\n",
    "for line in linesOfTestDataAfterPreProcessing:\n",
    "    newLine = line\n",
    "    for w in newLine.split():\n",
    "            newLine = newLine.replace(w, st.stem(w)) # for stemming\n",
    "    linesOfTestDataAfterSteming.append(newLine)\n",
    "    \n",
    "linesOfTestDataAfterStemingWithoutPunctuation = []\n",
    "for line in linesOfTestDataAfterSteming:\n",
    "    newLine = line\n",
    "    for w in newLine.split():\n",
    "         if w in string.punctuation:\n",
    "            newLine = newLine.translate(string.punctuation)\n",
    "            #newLine = newLine.replace(w,re.sub(r'[^\\w\\s]()','',w)) # for Punctuation \n",
    "    linesOfTestDataAfterStemingWithoutPunctuation.append(newLine)\n",
    "    \n",
    "    \n",
    "\n",
    "linesOfTestData = linesOfTestDataAfterStemingWithoutPunctuation\n",
    "\n",
    "print (\"\\n\\nFirst line after cleanup from Test Data: \",linesOfTestData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 227)\t0.379669374364\n",
      "  (0, 1039)\t0.173179023624\n",
      "  (0, 1601)\t0.221129975421\n",
      "  (0, 901)\t0.120934317987\n",
      "  (0, 34)\t0.120150423191\n",
      "  (0, 1957)\t0.214846297115\n",
      "  (0, 537)\t0.121842327064\n",
      "  (0, 1927)\t0.0914533476689\n",
      "  (0, 1258)\t0.125835788902\n",
      "  (0, 1253)\t0.281291963663\n",
      "  (0, 1909)\t0.106492061891\n",
      "  (0, 1905)\t0.201544539545\n",
      "  (0, 664)\t0.182468599301\n",
      "  (0, 1556)\t0.172930822947\n",
      "  (0, 1029)\t0.148716947762\n",
      "  (0, 928)\t0.218080682758\n",
      "  (0, 2095)\t0.140506794551\n",
      "  (0, 2037)\t0.436161365515\n",
      "  (0, 1215)\t0.130860128717\n",
      "  (0, 962)\t0.29614047614\n",
      "  (0, 729)\t0.13529277059\n",
      "  (0, 1621)\t0.134051335106\n",
      "  (0, 1571)\t0.114365950127\n",
      "  (0, 124)\t0.134939693324\n",
      "  (0, 1988)\t0.12795041138\n"
     ]
    }
   ],
   "source": [
    "#     vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "#                              tokenizer = None,    \\\n",
    "#                              preprocessor = None, \\\n",
    "#                              stop_words = None,   \\\n",
    "#                              max_features = 5000)\n",
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 50, stop_words = 'english')\n",
    "    \n",
    "train_data_features = vectorizer.fit_transform(linesOfTrainData)\n",
    "print(train_data_features[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "test_data_features = vectorizer.fit_transform(linesOfTestData)\n",
    "#print(test_data_features[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '10 minut', '10 mon', '10 mon old', '10 month', '100', '11', '12', '13', '14', '15', '16', '17', '18', '18 mon', '18 mon old', '18 month', '19', '1st', '20', '20 minut', '200', '21', '22', '24', '25', '2nd', '30', '34', '3rd', '40', '45', '50', '60', '90', '99', 'abl', 'abl sit', 'absolv', 'absolv lov', 'absorb', 'access', 'accid', 'accord', 'act', 'ad', 'adapt', 'addit', 'addition', 'adher', 'adjust', 'adjustable', 'admit', 'adorable', 'adult', 'adv', 'advert', 'affect', 'afford', 'afraid', 'ag', 'age', 'ago', 'agr', 'ahead', 'air', 'air dry', 'allow', 'alon', 'alot', 'altern', 'alway', 'amaz', 'amazing', 'amazon', 'amazon com', 'angl', 'anim', 'annoy', 'annoying', 'anoth', 'anoth brand', 'anoth review', 'answ', 'anym', 'anymore', 'anyon', 'anyth', 'anyth els', 'anywh', 'ap', 'apart', 'app', 'appear', 'apply', 'apprecy', 'ar', 'area', 'aren', 'arm', 'arms', 'arrived', 'ask', 'asleep', 'aspect', 'assembl', 'assemble', 'assum', 'attach', 'attempt', 'attract', 'autom', 'av', 'av bottl', 'avail', 'avent', 'avoid', 'aw', 'away', 'awesom', 'awesome', 'awful', 'awkward', 'babies', 'baby', 'baby baby', 'baby bjorn', 'baby born', 'baby bottl', 'baby boy', 'baby carry', 'baby didn', 'baby doesn', 'baby don', 'baby easy', 'baby food', 'baby gift', 'baby girl', 'baby hat', 'baby head', 'baby item', 'baby lik', 'baby lov', 'baby monit', 'baby month', 'baby month old', 'baby nee', 'baby produc', 'baby real', 'baby room', 'baby sit', 'baby sleep', 'baby start', 'baby stil', 'baby warm', 'backpack', 'bad', 'bag', 'bags', 'bal', 'bang', 'bar', 'bas', 'base', 'basket', 'bassinet', 'bat', 'bath', 'bath tim', 'bath toy', 'bath tub', 'bathroom', 'bathtub', 'batteries', 'battery', 'bear', 'beat', 'beauty', 'becam', 'becom', 'bed', 'bedroom', 'beep', 'beg', 'begin', 'beginning', 'bel', 'believ', 'belt', 'bend', 'benefit', 'bent', 'besid', 'best', 'best thing', 'bet', 'bet qual', 'better', 'bib', 'bibs', 'big', 'big baby', 'big deal', 'bigger', 'biggest', 'bin', 'bir', 'birthday', 'bit', 'bjorn', 'black', 'blanket', 'blankets', 'blend', 'block', 'blow', 'blu', 'blue', 'board', 'body', 'boil', 'book', 'boost', 'boost seat', 'boppy', 'bor', 'born', 'bottl', 'bottl baby', 'bottl brush', 'bottl leak', 'bottl nippl', 'bottl warm', 'bottl work', 'bottle', 'bottles', 'bottls', 'bought', 'bought anoth', 'bought baby', 'bought daught', 'bought son', 'bount', 'bowl', 'box', 'boy', 'bpa', 'bpa fre', 'bpa free', 'bra', 'brand', 'brand new', 'brands', 'break', 'breast', 'breast fee', 'breast milk', 'breast pump', 'breastf', 'breastfee', 'breastmilk', 'breath', 'bright', 'bright col', 'bring', 'bristl', 'britax', 'brok', 'broke', 'brought', 'brown', 'brown bottl', 'bru', 'brush', 'bubbl', 'buck', 'buckl', 'bug', 'build', 'built', 'bulky', 'bum', 'bumkin', 'bump', 'bunch', 'burn', 'burp', 'burp cloth', 'busy', 'button', 'buy', 'buy anoth', 'buy baby', 'buy new', 'buy produc', 'cabinet', 'cal', 'calm', 'cam', 'camer', 'canop', 'cap', 'car', 'car seat', 'card', 'carpet', 'carrier', 'carry', 'carry baby', 'cars', 'carseat', 'cart', 'cas', 'case', 'cat', 'catch', 'catch food', 'caught', 'caus', 'cel', 'cent', 'certain', 'chair', 'chairs', 'champ', 'chang', 'chang pad', 'chang tabl', 'chang table', 'change', 'chant', 'charg', 'cheap', 'cheaper', 'cheaply', 'cheaply mad', 'check', 'chem', 'chest', 'chew', 'chicco', 'child', 'childr', 'children', 'chin', 'cho', 'choice', 'chok', 'choos', 'chos', 'christmas', 'circ', 'city', 'claim', 'class', 'cle', 'cle easy', 'clean', 'cleaning', 'clear', 'cleing', 'click', 'climb', 'clip', 'clo', 'clo diap', 'clos', 'close', 'closed', 'closet', 'cloth', 'clothes', 'coff', 'col', 'cold', 'collaps', 'collect', 'color', 'colors', 'com', 'com apart', 'combin', 'comfort', 'comfort baby', 'comfortabl', 'comfortable', 'comfy', 'common', 'comp', 'compact', 'company', 'comparison', 'compart', 'complain', 'complaint', 'complaints', 'complet', 'completely', 'comply', 'conceiv', 'concept', 'concern', 'confid', 'confus', 'connect', 'cons', 'consid', 'consist', 'const', 'construct', 'consum', 'cont', 'contact', 'contain', 'continu', 'conto', 'control', 'convenient', 'conveny', 'convert', 'cool', 'cord', 'corn', 'correct', 'correctly', 'cost', 'cotton', 'couldn', 'count', 'coupl', 'coupl month', 'coupl tim', 'coupl week', 'cours', 'course', 'cov', 'cover', 'covs', 'cozy', 'crack', 'cradl', 'crap', 'crawl', 'crazy', 'cre', 'cream', 'cri', 'crib', 'crib mattress', 'crib sheet', 'crying', 'cub', 'cup', 'cup hold', 'cups', 'cur', 'curv', 'cush', 'custom', 'custom serv', 'cut', 'cute', 'cyc', 'dad', 'dai', 'dam', 'dang', 'dark', 'daught', 'daught born', 'daught lik', 'daught lov', 'daught month', 'daught month old', 'daughter', 'day', 'dayc', 'days', 'deal', 'dec', 'decid', 'decid buy', 'decid giv', 'decid try', 'deep', 'def', 'defect', 'defin', 'definit', 'definit recommend', 'definit wor', 'degr', 'del', 'depend', 'describ', 'design', 'design flaw', 'designed', 'desp', 'despit', 'detach', 'dev', 'develop', 'diap', 'diap bag', 'diap champ', 'diap chang', 'diap geny', 'diap pail', 'diaper', 'diapers', 'diaps', 'did', 'didn', 'didn ev', 'didn fit', 'didn know', 'didn lik', 'didn real', 'didn think', 'didn want', 'didn work', 'didnt', 'died', 'diff', 'diff brand', 'difference', 'different', 'difficul', 'difficult', 'din', 'direct', 'dirty', 'dirty diap', 'disappoint', 'disappointed', 'disappointing', 'discov', 'dish', 'dishwash', 'dishwasher', 'dispens', 'display', 'dispos', 'distract', 'does', 'doesn', 'doesn ev', 'doesn fit', 'doesn lik', 'doesn mind', 'doesn real', 'doesn stay', 'doesn tak', 'doesn work', 'doesnt', 'dog', 'doll', 'don', 'don buy', 'don ev', 'don feel', 'don fit', 'don know', 'don leak', 'don lik', 'don mind', 'don nee', 'don real', 'don recommend', 'don think', 'don understand', 'don want', 'don wast', 'don wast money', 'don work', 'don worry', 'dont', 'door', 'doorway', 'doubl', 'doubt', 'downsid', 'dr', 'dr brown', 'dr brown bottl', 'drag', 'drain', 'draw', 'drawback', 'dream', 'dress', 'dri', 'dril', 'drink', 'drip', 'driv', 'drool', 'drop', 'drop ins', 'dry', 'dryer', 'duck', 'dump', 'dur', 'durable', 'ear', 'eas', 'easier', 'easily', 'easy', 'easy adjust', 'easy baby', 'easy carry', 'easy cle', 'easy clean', 'easy fold', 'easy hold', 'easy instal', 'easy install', 'easy op', 'easy remov', 'easy set', 'easy tak', 'easy use', 'easy wash', 'easy wip', 'eat', 'eating', 'edg', 'edit', 'effect', 'effort', 'eith', 'elast', 'elect', 'elect pump', 'els', 'email', 'end', 'end buy', 'end return', 'enjoy', 'entertain', 'entir', 'ergo', 'espec', 'especially', 'ess', 'ev', 'ev tri', 'evenflo', 'everyday', 'everyon', 'everyth', 'everytim', 'everywh', 'evy', 'exact', 'example', 'exceiv', 'excel', 'excellent', 'excess', 'exchang', 'excit', 'exclud', 'expect', 'expected', 'expend', 'expensive', 'experience', 'expery', 'explain', 'expos', 'express', 'extend', 'extr', 'extrem', 'ey', 'fabr', 'fabric', 'fac', 'face', 'fact', 'fad', 'fail', 'fair', 'fal', 'fal apart', 'fal asleep', 'family', 'famy', 'fan', 'fantast', 'fantastic', 'far', 'fast', 'fast flow', 'faucet', 'fault', 'fav', 'favorit', 'favorite', 'fear', 'feat', 'feature', 'fed', 'fee', 'fee baby', 'feeding', 'feel', 'feel comfort', 'feel lik', 'feet', 'fel', 'felt', 'felt lik', 'fig', 'fil', 'fin', 'finally', 'fine', 'fing', 'firm', 'fish', 'fish pric', 'fisher', 'fit', 'fit baby', 'fit crib', 'fit gre', 'fit nic', 'fit perfect', 'fit perfectly', 'fit snug', 'fit wel', 'fiv', 'fix', 'flang', 'flannel', 'flap', 'flat', 'flaw', 'fleec', 'flex', 'flimsy', 'flip', 'flo', 'floor', 'flop', 'flow', 'flow nippl', 'fly', 'foam', 'fold', 'follow', 'food', 'foot', 'forc', 'forev', 'forget', 'form', 'formul', 'formula', 'forward', 'fram', 'fre', 'freak', 'free', 'freez', 'frequ', 'fresh', 'fridg', 'friend', 'friendly', 'friends', 'froz', 'fruit', 'frust', 'frustrating', 'ful', 'ful siz', 'fun', 'funct', 'functional', 'funny', 'furnit', 'fuss', 'fussy', 'fut', 'gap', 'garb', 'gas', 'gat', 'gats', 'gav', 'gav star', 'gear', 'gen', 'gentl', 'geny', 'gerb', 'giant', 'gift', 'girl', 'giv', 'giv baby', 'giv star', 'giv stars', 'giv try', 'glad', 'glad did', 'glass', 'glu', 'goe', 'going', 'gon', 'good', 'good ide', 'good idea', 'good produc', 'good qual', 'good quality', 'good review', 'good siz', 'good thing', 'got', 'got baby', 'grab', 'graco', 'grandma', 'grandson', 'gre', 'gre baby', 'gre buy', 'gre ide', 'gre idea', 'gre littl', 'gre pric', 'gre price', 'gre produc', 'gre product', 'gre way', 'great', 'green', 'grew', 'grip', 'grocery', 'gross', 'ground', 'grow', 'guard', 'guess', 'guy', 'gym', 'hadn', 'hair', 'half', 'halo', 'hammock', 'hand', 'hand fre', 'hand wash', 'handl', 'handle', 'hands', 'handy', 'hang', 'hang dry', 'hap', 'happen', 'happy', 'happy purchase', 'har', 'hard', 'hard cle', 'hard plast', 'hard tim', 'hardw', 'hasn', 'hassl', 'hassle', 'hat', 'haven', 'haven problem', 'hazard', 'head', 'headrest', 'hear', 'heard', 'heat', 'heavy', 'height', 'held', 'help', 'help baby', 'help sleep', 'helpful', 'hesit', 'hid', 'high', 'high chair', 'high hop', 'high qual', 'high recommend', 'highchair', 'highest', 'hip', 'hit', 'hol', 'hold', 'hold baby', 'hold bottl', 'hold wel', 'hole', 'holes', 'hom', 'home', 'honest', 'honstly', 'hook', 'hop', 'horr', 'horrible', 'hospit', 'hospital', 'hot', 'hot wat', 'hour', 'hours', 'hous', 'house', 'howev', 'hubby', 'hug', 'huge', 'hung', 'hungry', 'hurt', 'husband', 'ic', 'id', 'ide', 'idea', 'im', 'imagin', 'immediately', 'immedy', 'import', 'imposs', 'impress', 'improv', 'inch', 'includ', 'increas', 'incred', 'ind', 'individ', 'inexpend', 'inf', 'inf car', 'inf seat', 'infant', 'inform', 'init', 'ins', 'insert', 'insid', 'inside', 'inst', 'instal', 'install', 'instead', 'instruct', 'instructions', 'intend', 'interf', 'introduc', 'invest', 'irrit', 'isn', 'issu', 'issue', 'issues', 'item', 'items', 'job', 'joke', 'jump', 'junk', 'just', 'kept', 'key', 'kick', 'kid', 'kid lov', 'kiddo', 'kids', 'kind', 'kit', 'kitch', 'kitch sink', 'kitchen', 'kne', 'knew', 'knob', 'knock', 'know', 'known', 'label', 'lack', 'lact', 'lansinoh', 'lap', 'larg', 'larg siz', 'large', 'lat', 'latch', 'later', 'laugh', 'laundry', 'law', 'lay', 'lay flat', 'lb', 'lbs', 'lead', 'leak', 'leaking', 'leaks', 'lean', 'learn', 'leav', 'left', 'leg', 'legs', 'leng', 'length', 'let', 'level', 'lid', 'lie', 'lif', 'life', 'lift', 'light', 'light weight', 'lightweight', 'lik', 'lik baby', 'lik bottl', 'lik crazy', 'lik fact', 'lik good', 'lik ide', 'lik look', 'lik new', 'lik produc', 'lik review', 'likd', 'like', 'liks', 'limit', 'lin', 'line', 'link', 'lip', 'liquid', 'list', 'lit', 'littl', 'littl baby', 'littl big', 'littl bit', 'littl boy', 'littl girl', 'littl guy', 'littl hand', 'little', 'liv', 'liv room', 'll', 'lo', 'load', 'loc', 'lock', 'lol', 'long', 'long tim', 'long time', 'longer', 'look', 'look forward', 'look good', 'look gre', 'look lik', 'look nic', 'look someth', 'looking', 'loop', 'loos', 'loose', 'los', 'lost', 'lot', 'loud', 'lov', 'lov baby', 'lov easy', 'lov fact', 'lov ide', 'lov look', 'lov lov', 'lov play', 'lov produc', 'lov product', 'lov toy', 'lovd', 'love', 'lovs', 'low', 'lowest', 'luck', 'lucky', 'lug', 'machin', 'machine', 'mad', 'mail', 'main', 'maj', 'mak', 'mak baby', 'mak difficult', 'mak easy', 'mak gre', 'mak hard', 'mak sur', 'maks', 'man', 'maneuv', 'manufact', 'mark', 'market', 'mass', 'mat', 'match', 'material', 'mattress', 'mattress pad', 'max', 'mayb', 'maybe', 'meal', 'mean', 'meant', 'meas', 'mech', 'med', 'medel', 'medel pump', 'meet', 'ment', 'mentiond', 'mesh', 'mess', 'messy', 'met', 'method', 'microwav', 'middl', 'middl night', 'middle', 'mil', 'mildew', 'milk', 'min', 'mind', 'minim', 'minut', 'minutes', 'mir', 'mirac', 'mirac blanket', 'miss', 'mistak', 'mix', 'mo', 'mo old', 'mobl', 'mod', 'model', 'moist', 'mold', 'mom', 'mommy', 'mon', 'mon old', 'mon old baby', 'mon old daught', 'mon old lov', 'mon old son', 'money', 'money buy', 'monit', 'monits', 'monkey', 'mons', 'month', 'month ago', 'month old', 'month old stil', 'month stil', 'months', 'mony', 'morn', 'mos', 'mot', 'moth', 'mother', 'mou', 'mount', 'mouth', 'mov', 'multipl', 'munchkin', 'mus', 'music', 'nail', 'nam', 'nap', 'narrow', 'nat', 'near', 'near imposs', 'neat', 'necess', 'necessary', 'neck', 'nee', 'nee buy', 'nee someth', 'need', 'needed', 'needless', 'needless say', 'neeed', 'nees', 'neg', 'neg review', 'neith', 'nerv', 'net', 'nev', 'nev problem', 'new', 'new baby', 'new mom', 'newborn', 'newborns', 'nic', 'nice', 'nicely', 'night', 'night light', 'nippl', 'nipple', 'nipples', 'nippls', 'nois', 'noise', 'noisy', 'non', 'norm', 'nos', 'note', 'noth', 'nuk', 'numb', 'nurs', 'nursery', 'obvy', 'oc', 'occas', 'od', 'oft', 'oh', 'ok', 'okay', 'old', 'old baby', 'old daught', 'old lik', 'old lov', 'old son', 'old start', 'old stil', 'older', 'ones', 'onlin', 'online', 'ons', 'op', 'op clos', 'open', 'opin', 'opinion', 'opt', 'option', 'options', 'orang', 'ord', 'org', 'origin', 'oth', 'otherw', 'ount', 'outgrew', 'outgrow', 'outlet', 'outsid', 'outside', 'ov', 'overal', 'overall', 'overnight', 'oxo', 'oz', 'pac', 'pack', 'pack play', 'package', 'pad', 'pad cov', 'paid', 'pail', 'pain', 'paint', 'pair', 'pan', 'panel', 'pant', 'pap', 'par', 'parent', 'parents', 'park', 'particul', 'parts', 'pass', 'past', 'pattern', 'pay', 'peac', 'peac mind', 'pee', 'peel', 'penny', 'peopl', 'people', 'perfect', 'perfect siz', 'perfectly', 'perhap', 'period', 'perm', 'person', 'phon', 'photo', 'pick', 'pict', 'picture', 'piec', 'piece', 'pieces', 'pil', 'pillow', 'pin', 'pinch', 'pink', 'plac', 'place', 'plain', 'plan', 'plast', 'plastic', 'plat', 'play', 'play toy', 'play yard', 'playtex', 'pleas', 'plenty', 'plenty room', 'plu', 'plug', 'plus', 'plush', 'pocket', 'point', 'pointless', 'pok', 'poop', 'poor', 'pop', 'port', 'posit', 'position', 'poss', 'possible', 'post', 'pot', 'potty', 'potty chair', 'potty seat', 'potty train', 'pouch', 'pound', 'pounds', 'pour', 'pow', 'powd', 'pract', 'pre', 'precy', 'pref', 'prefer', 'pregn', 'pregnant', 'prep', 'pres', 'press', 'pretty', 'pretty easy', 'pretty good', 'prev', 'prevy', 'pri', 'pric', 'price', 'pricey', 'prim', 'print', 'pro', 'prob', 'problem', 'problems', 'process', 'produc', 'produc work', 'producs', 'product', 'products', 'project', 'proof', 'prop', 'properly', 'pros', 'protect', 'prov', 'provid', 'publ', 'pul', 'pump', 'pump styl', 'pump work', 'pumping', 'pur', 'purchas', 'purchas anoth', 'purchas item', 'purchase', 'purpos', 'purpose', 'push', 'qual', 'quality', 'queen', 'quick', 'quickly', 'quiet', 'quit', 'quit bit', 'rack', 'rail', 'rain', 'rais', 'ran', 'random', 'rang', 'rar', 'rash', 'rat', 'rath', 'rattl', 'rav', 'reach', 'read', 'read review', 'read reviews', 'ready', 'real', 'real don', 'real easy', 'real enjoy', 'real good', 'real gre', 'real hard', 'real help', 'real lik', 'real lov', 'real nee', 'real nic', 'real real', 'real want', 'real wel', 'real work', 'really', 'rear', 'reason', 'reasons', 'rec', 'receiv', 'receiv blanket', 'receiv gift', 'reclin', 'recommend', 'recommend anyon', 'recommend buy', 'recommend produc', 'recommend product', 'recommended', 'red', 'reduc', 'refil', 'reflux', 'refund', 'refus', 'reg', 'regard', 'regardless', 'regret', 'regul', 'rel', 'relax', 'releas', 'reliev', 'rely', 'remain', 'rememb', 'remot', 'remov', 'rep', 'replac', 'requir', 'research', 'resist', 'rest', 'resta', 'result', 'return', 'return item', 'reus', 'review', 'review said', 'review say', 'reviews', 'rid', 'ridic', 'right', 'right away', 'rigid', 'ring', 'rins', 'rip', 'risk', 'road', 'rock', 'rol', 'room', 'rot', 'rough', 'round', 'rub', 'ruin', 'run', 'sack', 'sad', 'saf', 'saf 1st', 'safe', 'safety', 'sag', 'said', 'sat', 'satisfy', 'sav', 'sav money', 'sav mony', 'saver', 'saw', 'say', 'scar', 'scent', 'scoop', 'scratch', 'scratches', 'scratchy', 'screaming', 'screen', 'screw', 'screwed', 'screws', 'seal', 'seam', 'search', 'seat', 'seats', 'sec', 'second', 'second child', 'seconds', 'sect', 'secure', 'seen', 'sel', 'select', 'self', 'send', 'sens', 'sensit', 'sent', 'sep', 'seriously', 'serv', 'serv purpos', 'sery', 'set', 'setting', 'settl', 'sev', 'sev diff', 'sev month', 'sev tim', 'sew', 'shad', 'shak', 'shap', 'shape', 'shar', 'sharp', 'sheep', 'sheet', 'shel', 'shelf', 'shield', 'shift', 'ship', 'shipping', 'shirt', 'shock', 'shop', 'shop cart', 'short', 'shouldn', 'shower', 'shown', 'shut', 'sick', 'sid', 'sid sid', 'sides', 'sids', 'sign', 'sil', 'silicon', 'simil', 'simpl', 'simple', 'simply', 'singl', 'sink', 'sint', 'sint baby', 'sint month', 'sint son', 'sippy', 'sippy cup', 'sippy cups', 'sist', 'sit', 'siz', 'siz bed', 'size', 'skin', 'skip', 'sleep', 'sleep night', 'sleep sack', 'sleeper', 'sleeping', 'sleepsack', 'sleev', 'slept', 'slid', 'slight', 'sling', 'slip', 'slippery', 'slot', 'slow', 'slow flow', 'smal', 'smal baby', 'smal siz', 'small', 'smel', 'smel lik', 'smell', 'smil', 'smoo', 'smooth', 'snack', 'snap', 'snug', 'soak', 'soap', 'soft', 'sold', 'solid', 'solution', 'solv', 'someon', 'someth', 'someth els', 'someth lik', 'sometim', 'sometims', 'somewh', 'son', 'son born', 'son lik', 'son lov', 'son month', 'son month old', 'son real', 'son start', 'song', 'soon', 'soooo', 'sooth', 'sor', 'sorry', 'sort', 'sound', 'sound lik', 'sounds', 'spac', 'space', 'spar', 'speak', 'spec', 'spee', 'spend', 'spend money', 'spent', 'spil', 'spin', 'spit', 'splash', 'spong', 'spoon', 'spot', 'spout', 'spray', 'spring', 'squ', 'squeak', 'squeez', 'stabl', 'stack', 'stag', 'stain', 'stair', 'stand', 'standard', 'standard siz', 'star', 'stars', 'start', 'stat', 'stay', 'stay plac', 'ste', 'steam', 'step', 'step stool', 'steril', 'stick', 'sticky', 'stiff', 'stil', 'stil fit', 'stil look', 'stil lov', 'stil work', 'stim', 'stink', 'stitch', 'stock', 'stomach', 'stool', 'stop', 'stop work', 'stor', 'storage', 'store', 'stores', 'story', 'straight', 'strange', 'strap', 'strapped', 'straps', 'straw', 'straws', 'strength', 'stretch', 'stretchy', 'strip', 'stroller', 'strollers', 'strong', 'struggle', 'stuck', 'stuff', 'stupid', 'sturdy', 'styl', 'style', 'success', 'suck', 'suct', 'suct cup', 'suction', 'suggest', 'suit', 'sum', 'sum inf', 'summer', 'sun', 'sup', 'sup easy', 'supply', 'support', 'suppos', 'sur', 'sure', 'surfac', 'surpr', 'swaddl', 'swaddl blanket', 'swe', 'swing', 'switch', 'tab', 'tabl', 'table', 'tag', 'tak', 'tak apart', 'tak baby', 'tak car', 'tak littl', 'tak long', 'tak lot', 'tak spac', 'tal', 'talk', 'tall', 'tap', 'target', 'teach', 'tear', 'tee', 'teeth', 'teething', 'tel', 'temp', 'tend', 'term', 'terr', 'terrible', 'test', 'text', 'th', 'thank', 'thanks', 'theref', 'thermomet', 'thing', 'thing don', 'thing lik', 'things', 'think', 'think gre', 'thought', 'thought gre', 'threw', 'thrilled', 'throw', 'throw away', 'throwing', 'thrown', 'thu', 'tie', 'tight', 'til', 'tilt', 'tim', 'tim baby', 'tim day', 'tim tri', 'time', 'times', 'tims', 'tiny', 'tip', 'tir', 'today', 'toddl', 'toddler', 'toddlers', 'togeth', 'toilet', 'toilet seat', 'told', 'ton', 'took', 'tool', 'toothbrush', 'toss', 'tot', 'touch', 'tough', 'towel', 'toy', 'toy bar', 'toys', 'track', 'tradit', 'train', 'training', 'transit', 'transport', 'trap', 'trash', 'travel', 'tray', 'tri', 'tri diff', 'tri sev', 'trick', 'tricky', 'tried', 'trim', 'trip', 'troubl', 'tru', 'true', 'trunk', 'trust', 'try', 'tub', 'tuck', 'tug', 'tummy', 'tummy tim', 'turn', 'turtl', 'tv', 'twic', 'twice', 'twin', 'twins', 'twist', 'typ', 'ultim', 'uncomfort', 'uncomfortable', 'undernea', 'underneath', 'understand', 'unfortun', 'unfortunately', 'unit', 'unless', 'unlik', 'upd', 'update', 'upright', 'upset', 'upsid', 'upstair', 'usd', 'use', 'used', 'useful', 'useless', 'usful', 'usless', 'uss', 'vac', 'valu', 'value', 'valv', 'vary', 've', 've ev', 've got', 've nev', 've seen', 've tri', 'velcro', 'vent', 'ventair', 'version', 'vert', 'vibr', 'video', 'video monit', 'view', 'vinyl', 'vis', 'visit', 'volum', 'vs', 'waist', 'wait', 'wak', 'wal', 'walk', 'wall', 'walmart', 'want', 'want lik', 'want lov', 'want someth', 'warm', 'warm wat', 'warmer', 'warn', 'warranty', 'wash', 'wash dry', 'wash wel', 'washed', 'washes', 'washing', 'wasn', 'wast', 'wast money', 'wast mony', 'waste', 'wat', 'watch', 'water', 'waterproof', 'way', 'way big', 'weak', 'wear', 'weath', 'websit', 'wedg', 'week', 'week ago', 'week old', 'weeks', 'weigh', 'weight', 'weird', 'wel', 'wel mad', 'wel wor', 'went', 'weren', 'wet', 'whatev', 'wheel', 'whenev', 'wheth', 'whit', 'whit nois', 'white', 'whol', 'whol thing', 'wid', 'wide', 'wif', 'wiggl', 'wil', 'wind', 'window', 'wint', 'wip', 'wip warm', 'wipes', 'wir', 'wish', 'wish bought', 'wok', 'wom', 'won', 'wond', 'wonderful', 'wont', 'wood', 'wor', 'wor money', 'wor mony', 'wor penny', 'wor price', 'word', 'work', 'work best', 'work bet', 'work better', 'work fin', 'work fine', 'work gre', 'work great', 'work lik', 'work perfect', 'work perfectly', 'work real', 'work wel', 'worked', 'working', 'works', 'world', 'worn', 'worry', 'wors', 'worse', 'worst', 'worthless', 'wouldn', 'wouldn buy', 'wouldn recommend', 'wrap', 'writ', 'writ review', 'wrong', 'wrot', 'yard', 'ye', 'yeah', 'year', 'year ago', 'year old', 'year old son', 'years', 'yellow', 'yes', 'yesterday', 'young', 'yr', 'yr old', 'zero', 'zip']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names() \n",
    "len(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 25575.\n"
     ]
    }
   ],
   "source": [
    "wordsInSet = set()\n",
    "wordsList= []\n",
    "\n",
    "wordsInSet =  set.intersection(wordsInTrainingSet,wordsInTestSet)\n",
    "wordsList = list(wordsInSet)\n",
    "\n",
    "print(\"Number of unique words: %d.\" % len(wordsInSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def createMatrices(train_data, test_data):\n",
    "    vectorizer = tv(ngram_range=(1, 2))\n",
    "    train_matrix = vectorizer.fit_transform(train_data)\n",
    "    test_matrix = vectorizer.transform(test_data)\n",
    "    return train_matrix, test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "createMatrices(linesOfTrainData,linesOfTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(linesOfTrainData, linesOfTestData, test_size= 0.1,train_size= 0.9, random_state=42)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "#linesOfTrainData_Transformed =  vectorizer.fit_transform(features_train)\n",
    "linesOfTrainData_Transformed =  vectorizer.fit_transform(linesOfTrainData)\n",
    "\n",
    "print (len(vectorizer.vocabulary_)) # after loading vocab from 1 source\n",
    "print(vectorizer.vocabulary_['younger'])\n",
    "linesOfTrainData_Transformed =  vectorizer.fit_transform(linesOfTestData)\n",
    "print (len(vectorizer.vocabulary_)) # after loading vocab from 2nd source\n",
    "print(vectorizer.vocabulary_['younger'])\n",
    "\n",
    "\n",
    "#linesOfTestData_Transformed = vectorizer.transform(features_test)\n",
    "linesOfTestData_Transformed = vectorizer.transform(linesOfTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "5115\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(wordsInSet)\n",
    "\n",
    "top_percentile = 0.2 # 0.2 Precentile\n",
    "features = c.most_common(int(round(len(c)*top_percentile)))\n",
    "features_counts = len(features)\n",
    "\n",
    "print(type(features))\n",
    "print(features_counts)\n",
    "\n",
    "#print (wordsList)\n",
    "\n",
    "#top_percentile = 0.5\n",
    "# features_counts = len(words.most_common(int(round(len(words)*top_percentile))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hoov', 1)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#print(features)\n",
    "print(features[:1][0])\n",
    "print(features[:1][0][1])\n",
    "\n",
    "featureList= []\n",
    "\n",
    "\n",
    "for feature in features:\n",
    "    featureList.append(feature[0])\n",
    "    \n",
    "   \n",
    "#print(featureList)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #Buid matrix\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    dim = len(featureList)\n",
    "    feature_set = set(featureList[:dim])\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        set_d = set(d)\n",
    "        \n",
    "        d = list(set.intersection(feature_set,set_d))\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        set_d = set(d)\n",
    "        \n",
    "        d = list(set.intersection(feature_set,set_d))\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 18506, ncols 5115, nnz 162538]\n",
      " [nrows 18506, ncols 5115, nnz 161779]\n"
     ]
    }
   ],
   "source": [
    "#print(linesOfTrainData[:1])\n",
    "docslinesOfTrainData = [l.split() for l in linesOfTrainData]\n",
    "matOflinesOfTrainData  = build_matrix(docslinesOfTrainData)\n",
    "csr_info(matOflinesOfTrainData)\n",
    "\n",
    "#print (matOflinesOfTrainData[:1])\n",
    "\n",
    "#print(linesOfTestData[:1])\n",
    "docslinesOfTestData = [l.split() for l in linesOfTestData]\n",
    "matOflinesOfTestData  = build_matrix(docslinesOfTestData)\n",
    "csr_info(matOflinesOfTestData)\n",
    "#print (matOflinesOfTestData[:1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# scale matrix and normalize its rows\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]] \n",
      "\n",
      "mat2: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]] \n",
      "\n",
      "mat3: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "mat2linesOfTrainData = csr_idf(matOflinesOfTrainData, copy=True)\n",
    "mat3linesOfTrainData = csr_l2normalize(mat2linesOfTrainData, copy=True)\n",
    "print(\"mat1:\", matOflinesOfTrainData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat2:\", mat2linesOfTrainData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat3:\", mat3linesOfTrainData[15,:20].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1: [[ 1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]] \n",
      "\n",
      "mat2: [[ 6.16228864  0.          0.          2.66655838  0.          0.          0.\n",
      "   0.          2.01712098  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]] \n",
      "\n",
      "mat3: [[ 0.59683403  0.          0.          0.25826326  0.          0.          0.\n",
      "   0.          0.19536353  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mat2linesOfTestData = csr_idf(matOflinesOfTestData, copy=True)\n",
    "mat3linesOfTestData = csr_l2normalize(mat2linesOfTestData, copy=True)\n",
    "print(\"mat1:\", matOflinesOfTestData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat2:\", mat2linesOfTestData[15,:20].todense(), \"\\n\")\n",
    "print(\"mat3:\", mat3linesOfTestData[15,:20].todense())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "linesOfTrainData_Transformed = mat3linesOfTrainData\n",
    "linesOfTestData_Transformed = mat3linesOfTestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(linesOfTrainData, linesOfTestData, test_size= 0.1,train_size= 0.9, random_state=42)\n",
    "\n",
    "\n",
    "# vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "# #linesOfTrainData_Transformed =  vectorizer.fit_transform(features_train)\n",
    "# linesOfTrainData_Transformed =  vectorizer.fit_transform(linesOfTrainData)\n",
    "\n",
    "# print (len(vectorizer.vocabulary_)) # after loading vocab from 1 source\n",
    "# print(vectorizer.vocabulary_['younger'])\n",
    "# linesOfTrainData_Transformed =  vectorizer.fit_transform(linesOfTestData)\n",
    "# print (len(vectorizer.vocabulary_)) # after loading vocab from 2nd source\n",
    "# print(vectorizer.vocabulary_['younger'])\n",
    "\n",
    "\n",
    "# #linesOfTestData_Transformed = vectorizer.transform(features_test)\n",
    "# linesOfTestData_Transformed = vectorizer.transform(linesOfTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['+1', 'thi', 'book', 'lif', 'saver.', 'help', 'abl', 'go', 'back', 'track', 'trends,', 'answ', 'pedy', 'questions,', 'commun', 'diff', 'tim', 'night', 'newborn.', 'think', 'on', 'thing', 'everyon', 'requir', 'leav', 'hospital.', 'went', 'pag', 'newborn', 'version,', 'mov', 'inf', 'version,', 'fin', 'second', 'inf', 'book', '(third', 'total)', 'right', 'baby', 'turn', '1.', 'see', 'thing', 'must', 'hav', 'baby', '[...]']]\n",
      "-------\n",
      "  (0, 0)\t0.447782242315\n",
      "  (0, 1)\t0.168321133294\n",
      "  (0, 2)\t0.170910911753\n",
      "  (0, 3)\t0.173108657998\n",
      "  (0, 4)\t0.334837738417\n",
      "  (0, 5)\t0.27912189538\n",
      "  (0, 6)\t0.164990673381\n",
      "  (0, 7)\t0.208615752492\n",
      "  (0, 8)\t0.230029276457\n",
      "  (0, 9)\t0.116232378237\n",
      "  (0, 10)\t0.451145028484\n",
      "  (0, 11)\t0.403999377432\n",
      "  (0, 12)\t0.134325525554\n"
     ]
    }
   ],
   "source": [
    "print(docslinesOfTrainData[:1]) \n",
    "print('-------')\n",
    "print(linesOfTrainData_Transformed[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['perfect', 'new', 'parents.', 'abl', 'keep', 'track', \"baby's\", 'feeding,', 'sleep', 'diap', 'chang', 'schedule', 'first', 'two', 'half', 'month', 'lif.', 'mad', 'lif', 'easy', 'doct', 'would', 'ask', 'quest', 'habit', 'right', 'there!']]\n",
      "-------\n",
      "  (0, 0)\t0.4715424567\n",
      "  (0, 1)\t0.458910443559\n",
      "  (0, 2)\t0.20628073245\n",
      "  (0, 3)\t0.204046834323\n",
      "  (0, 4)\t0.326373563928\n",
      "  (0, 5)\t0.159194725122\n",
      "  (0, 6)\t0.174704857782\n",
      "  (0, 7)\t0.544658832896\n",
      "  (0, 8)\t0.154351449046\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(docslinesOfTestData[:1])\n",
    "print('-------')\n",
    "print(linesOfTestData_Transformed[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selector = SelectPercentile(f_classif, percentile=10)\n",
    "# selector.fit(linesOfTrainData_Transformed, labels_train) #labels_train\n",
    "# linesOfTrainData_Transformed = selector.transform(linesOfTrainData_Transformed)\n",
    "# linesOfTestData_Transformed = selector.transform(linesOfTestData_Transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def CalculateCosine(vt,vs):\n",
    "#         dotProduct = vt.dot(np.transpose(vS))#######\n",
    "#         lengtht = np.linalg.norm(vt.data)\n",
    "#         lengthS = np.linalg.norm(vS.data)\n",
    "\n",
    "#         #handle exceptions\n",
    "\n",
    "#         if lengthS!=0 and lengtht!=0 :\n",
    "#             cosineSimilarityValue= dotProduct/(lengtht*lengthS)\n",
    "#         else:\n",
    "#             cosineSimilarityValue= 0\n",
    "#         return cosineSimilarityValue\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def CalculateCosine(vt,vs):\n",
    "        cosineSimilarityValue = cosine_similarity(vt,vs)\n",
    "        return cosineSimilarityValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cosineSimilarityValues' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-48ffd3cca364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcosineSimilarityValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalculateCosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinesOfTestData_Transformed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinesOfTrainData_Transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosineSimilarityValues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cosineSimilarityValues' is not defined"
     ]
    }
   ],
   "source": [
    "cosineSimilarityValue = CalculateCosine(linesOfTestData_Transformed,linesOfTrainData_Transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18506\n"
     ]
    }
   ],
   "source": [
    "print(len(cosineSimilarityValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count :  18506\n",
      "--The End--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = open('TestData/Test/format_out.dat', 'w')\n",
    "count = 0\n",
    "for row in cosineSimilarityValue:\n",
    "\n",
    "    \n",
    "    #kneighbours = heapq.nlargest(5, row)\n",
    "    k=72\n",
    "    partitioned_row_byindex = np.argpartition(-row, k)  \n",
    "    similar_index = partitioned_row_byindex[:k]\n",
    "    \n",
    "    #print(similar_index)\n",
    "    \n",
    "    neighbourReviewTypeList = []\n",
    "    neighbourReviewTypeNegative = 0\n",
    "    neighbourReviewTypePositive = 0\n",
    "    \n",
    "    #print(\"@@@@@\",+count)\n",
    "    for index in similar_index:\n",
    "\n",
    "        if linesOfTrainData[index].strip()[0] == '-':\n",
    "            #neighbourReviewTypeList.append(\"-1\")\n",
    "            neighbourReviewTypeNegative+=1\n",
    "        elif linesOfTrainData[index].strip()[0] == '+':\n",
    "            #neighbourReviewTypeList.append(\"+1\")\n",
    "            neighbourReviewTypePositive+=1\n",
    "            \n",
    "    \n",
    "    if neighbourReviewTypeNegative > neighbourReviewTypePositive:\n",
    "        f.write('-1\\n')\n",
    "        count+=1\n",
    "    else:\n",
    "        f.write('+1\\n')\n",
    "        count+=1\n",
    "\n",
    "print(\"count : \",count)\n",
    "print(\"--The End--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 63829.\n"
     ]
    }
   ],
   "source": [
    "# get a frequency count for all words in the Training docs\n",
    "\n",
    "\n",
    "\n",
    "wordsInTrainingSet = set()\n",
    "for d in linesOfTrainData:\n",
    "    for w in d.split():\n",
    "        if w == \"+1\" or w == \"-1\":\n",
    "            continue\n",
    "        else:\n",
    "            if w not in wordsInTrainingSet:\n",
    "                wordsInTrainingSet.add(w)\n",
    "                #wordsList.append(w)\n",
    "            #else:\n",
    "             #   wordsInTrainingSet[w] += 1\n",
    "print(\"Number of unique words: %d.\" % len(wordsInTrainingSet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
